{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cdfc37",
   "metadata": {},
   "source": [
    "# Repare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd679af4",
   "metadata": {},
   "source": [
    "## txt2csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "input_file = './dataset/ps_encodedcommand_data.txt'\n",
    "output_file = './dataset/ps_encodedcommand_data.csv'\n",
    "\n",
    "with open(input_file, 'rb') as file:\n",
    "    content = file.read()\n",
    "\n",
    "content_str = content.decode('latin-1')\n",
    "\n",
    "pattern = re.compile(\n",
    "        r'######################### START #########################(.*?)#########################  END  #########################',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "matches = pattern.findall(content_str)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for match in matches:\n",
    "    original_code = re.search(r'\\[Original Code\\]\\n\\n(.*?)\\n\\n', match, re.DOTALL).group(1).strip()\n",
    "    filename = re.search(r'\\[Filename\\]\\n\\n(.*?)\\n\\n', match, re.DOTALL).group(1).strip()\n",
    "    arguments = re.search(r'\\[Arguments\\]\\n\\n(.*?)\\n\\n', match, re.DOTALL).group(1).strip()\n",
    "    b64_decoded = re.search(r'\\[B64 Decoded\\]\\n\\n(.*?)\\n\\n', match, re.DOTALL).group(1).strip()\n",
    "    family_name = re.search(r'\\[Family Name\\]\\n\\n(.*?)\\n\\n', match, re.DOTALL).group(1).strip()\n",
    "\n",
    "    data = {\n",
    "        'Original Code': original_code,\n",
    "        'Filename': filename, \n",
    "        'Arguments': arguments,\n",
    "        'B64 Decoded': b64_decoded,\n",
    "        'Family Name': family_name\n",
    "    }\n",
    "    data_list.append(data)\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Original Code', 'Filename', 'Arguments', 'B64 Decoded', 'Family Name']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for data in data_list:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f'Data has been written to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d18b1f",
   "metadata": {},
   "source": [
    "## preprocess CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a877a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from dataclasses import replace\n",
    "\n",
    "input_file = './dataset/ps_encodedcommand_data.csv'\n",
    "output_file = './dataset/data_processed.csv'\n",
    "\n",
    "try: \n",
    "    df = pd.read_csv(input_file)\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "def extract_executable(input_string):\n",
    "    pattern = re.compile(r'\\\\\\\"(.*?)\\\\\\\"|(\\S+\\.exe)|\\bpowershell\\b', re.IGNORECASE)\n",
    "    match = pattern.search(input_string)\n",
    "    if match:\n",
    "        if match.group(1):\n",
    "            result = match.group(1)\n",
    "        elif match.group(2):\n",
    "            result = match.group(2)\n",
    "        else:\n",
    "            result = match.group(0)\n",
    "        result = result.replace('\"', '')\n",
    "        return result\n",
    "    return None\n",
    "\n",
    "# √Åp d·ª•ng h√†m extract_executable l√™n c·ªôt 'Original Code' v√† t·∫°o c·ªôt m·ªõi 'Path Exec'\n",
    "df['Path Exec'] = df['Original Code'].apply(extract_executable)\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "def combine_attributes(row):\n",
    "    Path_Exec = str(row['Path Exec'])\n",
    "    Arguments = re.sub(r\"[\\[\\]',]\", '', str(row['Arguments']))\n",
    "    Payload = str(row['B64 Decoded'])\n",
    "\n",
    "    combine_value = f'{Path_Exec} {Arguments} {Payload}'\n",
    "    return combine_value\n",
    "\n",
    "df['Full Payload'] = df.apply(combine_attributes, axis=1)\n",
    "print(df['Family Name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083cd00",
   "metadata": {},
   "source": [
    "## Combine Family name to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = './dataset/data_processed.csv'\n",
    "output_file = './dataset/data_labeled.csv'\n",
    "\n",
    "try: \n",
    "    df = pd.read_csv(input_file)\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "def map_family_to_label(family_name):\n",
    "    if pd.isnull(family_name):\n",
    "        return 'Unknown'\n",
    "\n",
    "    family_name = family_name.lower()\n",
    "\n",
    "    # Bypass\n",
    "    if 'bypass' in family_name or 'remove av' in family_name:\n",
    "        return 'Bypass'\n",
    "\n",
    "    # TaskExecution\n",
    "    elif 'task' in family_name or 'scheduled' in family_name or 'com' in family_name \\\n",
    "        or 'bits' in family_name or 'vb task' in family_name \\\n",
    "        or 'dynamite' in family_name:\n",
    "        return 'TaskExecution'\n",
    "\n",
    "    # Downloader\n",
    "    elif 'downloader' in family_name or 'transfer' in family_name or 'proxy' in family_name \\\n",
    "        or 'iexds' in family_name or 'dfsp' in family_name:\n",
    "        return 'Downloader'\n",
    "\n",
    "    # Injector\n",
    "    elif 'inject' in family_name or 'trojan' in family_name or 'shellcode' in family_name \\\n",
    "        or 'remote dll' in family_name or 'rhttp' in family_name \\\n",
    "        or ('unicorn' in family_name and 'modified' in family_name):\n",
    "        return 'Injector'\n",
    "\n",
    "    # Payload\n",
    "    elif 'power' in family_name or 'empire' in family_name or 'veil' in family_name \\\n",
    "        or 'txt c2' in family_name or ('unicorn' in family_name and 'modified' not in family_name):\n",
    "        return 'Payload'\n",
    "\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# G√°n nh√£n\n",
    "df['Label'] = df['Family Name'].apply(map_family_to_label)\n",
    "\n",
    "# Lo·∫°i b·ªè 'Unknown'\n",
    "row_before = len(df)\n",
    "df = df[df['Label'] != 'Unknown']\n",
    "row_after = len(df)\n",
    "row_removed = row_before - row_after\n",
    "print(f'Number of rows removed: {row_removed}')\n",
    "\n",
    "# Th·ªëng k√™ s·ªë l∆∞·ª£ng nh√£n\n",
    "label_counts = df['Label'].value_counts()\n",
    "print('Label counts:')\n",
    "print(label_counts)\n",
    "\n",
    "# Th·ªëng k√™ t·ª∑ l·ªá ph·∫ßn trƒÉm\n",
    "label_percentages = (label_counts / len(df)) * 100\n",
    "print('Label percentages:')\n",
    "print(label_percentages)\n",
    "\n",
    "# L∆∞u file\n",
    "columns_to_keep = ['Full Payload', 'Label']\n",
    "df = df[columns_to_keep]\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f'DataFrame has been written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c758ac",
   "metadata": {},
   "source": [
    "# extend dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ===== C√°c nh√£n h·ª£p l·ªá theo PowerDetector =====\n",
    "VALID_LABELS = {'Bypass', 'Downloader', 'Injector', 'Payload', 'TaskExecution'}\n",
    "\n",
    "def append_csv_files(base_df, csv_file_paths):\n",
    "    \"\"\"\n",
    "    N·ªëi th√™m c√°c file CSV v√†o base_df, gi·ªØ l·∫°i c·ªôt 'Full Payload' l√†m chu·∫©n.\n",
    "    T·ª± g√°n 'Payload' = 'Full Payload' sau khi ki·ªÉm tra h·ª£p l·ªá.\n",
    "    \"\"\"\n",
    "    total_new_samples = 0\n",
    "    total_skipped = 0\n",
    "\n",
    "    for file_path in csv_file_paths:\n",
    "        try:\n",
    "            df_new = pd.read_csv(file_path)\n",
    "\n",
    "            print(f\"\\nüìä Ph√¢n ph·ªëi nh√£n tr∆∞·ªõc khi n·ªëi (file: {file_path}):\")\n",
    "            print(df_new['Label'].value_counts())\n",
    "\n",
    "            # Chu·∫©n h√≥a: ∆∞u ti√™n d√πng 'Full Payload'\n",
    "            if 'Full Payload' not in df_new.columns:\n",
    "                if 'Payload' in df_new.columns:\n",
    "                    df_new['Full Payload'] = df_new['Payload']\n",
    "            else:\n",
    "                df_new = df_new.drop(columns=['Payload'], errors='ignore')\n",
    "\n",
    "            # G√°n Payload = Full Payload\n",
    "            df_new['Payload'] = df_new['Full Payload']\n",
    "\n",
    "            # Ki·ªÉm tra c·∫•u tr√∫c\n",
    "            if 'Payload' not in df_new.columns or 'Label' not in df_new.columns:\n",
    "                print(f\"‚ö†Ô∏è File {file_path} kh√¥ng h·ª£p l·ªá (thi·∫øu Payload ho·∫∑c Label). B·ªè qua.\")\n",
    "                continue\n",
    "\n",
    "            # L·ªçc 5 nh√£n ch√≠nh\n",
    "            before = len(df_new)\n",
    "            df_new = df_new[df_new['Label'].isin(VALID_LABELS)]\n",
    "            skipped = before - len(df_new)\n",
    "            total_skipped += skipped\n",
    "\n",
    "            print(f\"\\nüìä Ph√¢n ph·ªëi nh√£n sau khi l·ªçc (file: {file_path}):\")\n",
    "            print(df_new['Label'].value_counts())\n",
    "\n",
    "            base_df = pd.concat([base_df, df_new], ignore_index=True)\n",
    "            total_new_samples += len(df_new)\n",
    "            print(f\"‚úÖ ƒê√£ n·ªëi {len(df_new)} d√≤ng t·ª´ {file_path}, b·ªè qua {skipped} d√≤ng kh√¥ng h·ª£p l·ªá.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói khi x·ª≠ l√Ω file {file_path}: {e}\")\n",
    "\n",
    "    return base_df, total_new_samples, total_skipped\n",
    "\n",
    "# ===== Main script =====\n",
    "base_dataset = './dataset/data_labeled.csv'\n",
    "new_dataset_files = [\n",
    "    './dataset_extra/dataset/amsi_fail.csv',\n",
    "    # './dataset_extra/dataset/Amsi-Bypass-Powershell.csv', \n",
    "    './dataset_extra/dataset/Malicious-PowerShell-Dataset_bazaar_out.csv',\n",
    "    './dataset_extra/dataset/Malicious-PowerShell-Dataset_hybrid_analysis_out.csv',\n",
    "    './dataset_extra/dataset/Malicious-PowerShell-Dataset_triage_out.csv',\n",
    "    './dataset_extra/dataset/mpsd.csv',\n",
    "    # './dataset_extra/dataset/others.csv',\n",
    "]  \n",
    "output_dataset = './dataset/data_labeled_extended.csv'\n",
    "\n",
    "# Load g·ªëc\n",
    "df_base = pd.read_csv(base_dataset)\n",
    "\n",
    "print(\"\\nüìä Ph√¢n ph·ªëi nh√£n trong dataset g·ªëc:\")\n",
    "print(df_base['Label'].value_counts())\n",
    "\n",
    "# N·ªëi th√™m\n",
    "df_extended, total_new, total_skipped = append_csv_files(df_base, new_dataset_files)\n",
    "\n",
    "# L∆∞u\n",
    "df_extended.to_csv(output_dataset, index=False)\n",
    "\n",
    "# ===== Th·ªëng k√™ =====\n",
    "print(f\"\\nüéØ Ho√†n t·∫•t: ƒê√£ th√™m {total_new} d√≤ng m·ªõi.\")\n",
    "print(f\"üö´ ƒê√£ lo·∫°i {total_skipped} d√≤ng c√≥ nh√£n kh√¥ng h·ª£p l·ªá.\")\n",
    "print(f\"üíæ Dataset m·ªü r·ªông ƒë√£ l∆∞u v√†o: {output_dataset}\")\n",
    "\n",
    "# ===== Visualize =====\n",
    "label_counts = df_extended['Label'].value_counts()\n",
    "print(\"\\nüìä Ph√¢n ph·ªëi nh√£n sau khi n·ªëi:\")\n",
    "print(label_counts)\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "plt.figure(figsize=(8, 5))\n",
    "label_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Ph√¢n ph·ªëi nh√£n trong dataset m·ªü r·ªông')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('S·ªë l∆∞·ª£ng m·∫´u')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7492ed",
   "metadata": {},
   "source": [
    "## deobfuscate 3 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ===== LIGHT DEOBFUSCATION LOGIC =====\n",
    "def remove_comments(script: str) -> str:\n",
    "    return re.sub(r'#.*', '', script)\n",
    "\n",
    "def flatten_string_operations(script: str) -> str:\n",
    "    pattern = r'\"([^\"]+)\"\\s*\\+\\s*\"([^\"]+)\"'\n",
    "    while re.search(pattern, script):\n",
    "        script = re.sub(pattern, lambda m: '\"' + m.group(1) + m.group(2) + '\"', script)\n",
    "    return script\n",
    "\n",
    "def simplify_token_obfuscation(script: str) -> str:\n",
    "    # Placeholder cho dynamic execution: .(...) ‚Üí Invoke-Expression\n",
    "    pattern = r\"\\.\\s*\\(\\s*\\$?[a-zA-Z0-9\\+\\[\\]\\.\\(\\)'\\\" _-]+\\s*\\)\"\n",
    "    script = re.sub(pattern, 'Invoke-Expression', script, flags=re.IGNORECASE)\n",
    "\n",
    "    replacements = {\n",
    "        r'\\bIEX\\b': 'Invoke-Expression',\n",
    "        r'\\bInVoKe-ExPrEsSiOn\\b': 'Invoke-Expression',\n",
    "        r'\\bNEW-OBJECT\\b': 'New-Object',\n",
    "        r'\\bNeW-ObJeCt\\b': 'New-Object',\n",
    "        r'\\bSPlit\\b': 'Split',\n",
    "        r'\\bJoin\\b': 'Join'\n",
    "    }\n",
    "    for pattern, replacement in replacements.items():\n",
    "        script = re.sub(pattern, replacement, script, flags=re.IGNORECASE)\n",
    "\n",
    "    return script\n",
    "\n",
    "def light_deobfuscate(script: str) -> str:\n",
    "    script = remove_comments(script)\n",
    "    script = flatten_string_operations(script)\n",
    "    script = simplify_token_obfuscation(script)\n",
    "    return script\n",
    "\n",
    "# ===== APPLY TO CSV =====\n",
    "input_file = './dataset/data_labeled_extended.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading {input_file}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Deobfuscate t·ª´ng d√≤ng v√† ghi ƒë√® v√†o c·ªôt Full Payload\n",
    "print(\"üîÅ Deobfuscating and overwriting Full Payload...\")\n",
    "df['Full Payload'] = df['Full Payload'].fillna('').apply(light_deobfuscate)\n",
    "\n",
    "# Ghi ƒë√® l·∫°i file c≈©\n",
    "df.to_csv(input_file, index=False)\n",
    "print(f\"‚úÖ Done. Updated file saved to: {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76322258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccecd57",
   "metadata": {},
   "source": [
    "# Extract vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe567557",
   "metadata": {},
   "source": [
    "## rela2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from node2vec import Node2Vec\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "input_file = './dataset/data_labeled_extended.csv'\n",
    "output_file = input_file\n",
    "rela2vec_dir = './vectorization/rela2vec'\n",
    "graphs_file = os.path.join(rela2vec_dir, 'semantic_graph_extended.pkl')\n",
    "os.makedirs(rela2vec_dir, exist_ok=True)\n",
    "\n",
    "# ============= LOAD DATA =============\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "if 'Rela2Vec' not in df.columns:\n",
    "    df['Rela2Vec'] = ''\n",
    "else:\n",
    "    df['Rela2Vec'] = df['Rela2Vec'].astype(str)\n",
    "\n",
    "# ============= BUILD GRAPHS =============\n",
    "def build_graph(idx, payload):\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nlp.max_length = 20_000_000  # h·ªó tr·ª£ vƒÉn b·∫£n d√†i\n",
    "        doc = nlp(payload)\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            G.add_node(ent.text, type=ent.label_)\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            ents = [ent.text for ent in sent.ents]\n",
    "            for i in range(len(ents)):\n",
    "                for j in range(i + 1, len(ents)):\n",
    "                    G.add_edge(ents[i], ents[j])\n",
    "\n",
    "        return idx, G\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error building graph at idx={idx}: {e}\")\n",
    "        return idx, nx.Graph()\n",
    "\n",
    "if os.path.exists(graphs_file):\n",
    "    print(f\"‚úÖ Found existing semantic graph at {graphs_file}. Loading...\")\n",
    "    with open(graphs_file, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "else:\n",
    "    print(\"‚è≥ No existing graph found. Building semantic graphs in parallel...\")\n",
    "    full_payloads = df['Full Payload'].dropna()\n",
    "    graphs = {}\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=16) as executor:\n",
    "        futures = {executor.submit(build_graph, idx, payload): idx for idx, payload in full_payloads.items()}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Building Graphs (Parallel)\"):\n",
    "            idx, G = future.result()\n",
    "            graphs[idx] = G\n",
    "\n",
    "    with open(graphs_file, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "    print(f\"‚úÖ Saved semantic graphs to {graphs_file}\")\n",
    "\n",
    "# ============= EMBEDDING =============\n",
    "def embed_graph(idx, graph_data):\n",
    "    try:\n",
    "        G = graph_data\n",
    "        if G is None or len(G.nodes) == 0:\n",
    "            return idx, ' '.join(['0'] * 128)\n",
    "\n",
    "        node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=4, quiet=True)\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "        node_embeddings = model.wv\n",
    "        vec = np.mean([node_embeddings[node] for node in G.nodes if node in node_embeddings], axis=0)\n",
    "        return idx, ' '.join(map(str, vec))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Node2Vec error at idx={idx}: {e}\")\n",
    "        return idx, ' '.join(['0'] * 128)\n",
    "\n",
    "missing_indices = [\n",
    "    i for i in range(len(df))\n",
    "    if pd.isna(df.at[i, 'Rela2Vec']) or str(df.at[i, 'Rela2Vec']).strip().lower() in ['nan', '']\n",
    "]\n",
    "\n",
    "print(f\"üöÄ Embedding {len(missing_indices)} graphs using ProcessPoolExecutor...\")\n",
    "batch_size = 100\n",
    "with ProcessPoolExecutor(max_workers=16) as executor:\n",
    "    futures = {executor.submit(embed_graph, idx, graphs.get(idx)): idx for idx in missing_indices}\n",
    "    completed = 0\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Embedding (ProcessPool)\"):\n",
    "        idx, vec = future.result()\n",
    "        df.at[idx, 'Rela2Vec'] = vec\n",
    "        completed += 1\n",
    "\n",
    "        if completed % batch_size == 0:\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"üíæ Auto-saved after {completed} records.\")\n",
    "\n",
    "# ============= SAVE FINAL =============\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Done! Rela2Vec vectors saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed9c4a",
   "metadata": {},
   "source": [
    "## char2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcf412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Improved: Generate Char2Vec only for missing records ======\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_char2vec_csv(input_csv, column_name='Full Payload', label_column='Label', vector_size=128, window=5, min_count=1, workers=4):\n",
    "    model_dir = './vectorization/char2vec'\n",
    "    model_path = os.path.join(model_dir, 'char2vec_extended.model')\n",
    "\n",
    "    # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {input_csv}: {e}\")\n",
    "        return\n",
    "\n",
    "    if column_name not in df.columns or label_column not in df.columns:\n",
    "        raise ValueError(f\"'{column_name}' ho·∫∑c '{label_column}' kh√¥ng t·ªìn t·∫°i trong file CSV.\")\n",
    "\n",
    "    # N·∫øu ch∆∞a c√≥ c·ªôt Char2Vec ‚ûî t·∫°o c·ªôt tr·ªëng\n",
    "    if 'Char2Vec' not in df.columns:\n",
    "        df['Char2Vec'] = np.nan\n",
    "\n",
    "    # Build Char2Vec model (d√π c√≥ record ƒë·∫ßy ƒë·ªß hay kh√¥ng c≈©ng c·∫ßn model ƒë·ªÉ embed m·ªõi)\n",
    "    sentences = [list(str(code)) for code in df[column_name] if isinstance(code, str)]\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    print(f\"‚úÖ ƒê√£ hu·∫•n luy·ªán xong Char2Vec v·ªõi {len(model.wv.index_to_key)} k√Ω t·ª±.\")\n",
    "\n",
    "    # Save model ƒë·ªÉ sau n√†y d√πng l·∫°i\n",
    "    model.save(model_path)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u model t·∫°i {model_path}\")\n",
    "\n",
    "    # Embed t·ª´ng record n·∫øu thi·∫øu\n",
    "    for idx in tqdm(range(len(df)), desc=\"Embedding missing Char2Vec\"):\n",
    "        if pd.notna(df.at[idx, 'Char2Vec']):\n",
    "            # N·∫øu ƒë√£ c√≥ vector r·ªìi th√¨ b·ªè qua\n",
    "            continue\n",
    "\n",
    "        payload = df.at[idx, column_name]\n",
    "\n",
    "        if not isinstance(payload, str):\n",
    "            char_vec = np.zeros(vector_size)\n",
    "        else:\n",
    "            vectors = [model.wv[char] for char in list(payload) if char in model.wv]\n",
    "            if vectors:\n",
    "                char_vec = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                char_vec = np.zeros(vector_size)\n",
    "\n",
    "        vec_str = ' '.join(map(str, char_vec))\n",
    "        df.at[idx, 'Char2Vec'] = vec_str\n",
    "\n",
    "    # Ghi ƒë√® l·∫°i CSV\n",
    "    df.to_csv(input_csv, index=False)\n",
    "    print(f\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t {input_csv} v·ªõi Char2Vec cho c√°c record c√≤n thi·∫øu.\")\n",
    "\n",
    "# ===========================\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "input_csv = './dataset/data_labeled_extended.csv'\n",
    "\n",
    "generate_char2vec_csv(input_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe4848",
   "metadata": {},
   "source": [
    "## token2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ddbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Improved: Generate Token2Vec only for missing records ======\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== Helper functions =====\n",
    "\n",
    "def extract_token(script):\n",
    "    cmdlets = re.findall(r'\\b(?:Invoke-Expression|Invoke-Command|Start-Process|New-Object|Set-ExecutionPolicy|Get-Content|Add-Type|Invoke-WebRequest|Invoke-RestMethod|Get-WmiObject|Out-File|New-Service|Remove-Item)\\b', script)\n",
    "    variables = re.findall(r'\\$[a-zA-Z_][\\w]*', script)\n",
    "    functions = re.findall(r'\\bfunction\\s+\\w+', script)\n",
    "    keywords = re.findall(r'\\b(?:if|else|for|while|return)\\b', script)\n",
    "    parameters = re.findall(r'-\\w+', script)\n",
    "    strings = re.findall(r'\".+?\"', script)\n",
    "    return cmdlets + variables + functions + keywords + parameters + strings\n",
    "\n",
    "def generate_token2vec(input_csv, column_name='Full Payload', label_column='Label', vector_size=128, window=5, min_count=1, workers=16):\n",
    "    model_dir = './vectorization/token2vec'\n",
    "    model_path = os.path.join(model_dir, 'token2vec_extended.model')\n",
    "\n",
    "    # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {input_csv}: {e}')\n",
    "        return\n",
    "\n",
    "    if column_name not in df.columns or label_column not in df.columns:\n",
    "        raise ValueError(f\"'{column_name}' ho·∫∑c '{label_column}' kh√¥ng t·ªìn t·∫°i trong file CSV.\")\n",
    "\n",
    "    # N·∫øu ch∆∞a c√≥ c·ªôt Token2Vec ‚ûî t·∫°o c·ªôt tr·ªëng\n",
    "    if 'Token2Vec' not in df.columns:\n",
    "        df['Token2Vec'] = np.nan\n",
    "\n",
    "    print(\"‚è≥ ƒêang ti·∫øn h√†nh build ho·∫∑c update Token2Vec...\")\n",
    "\n",
    "    # Extract token lists cho to√†n b·ªô payloads\n",
    "    token_lists = []\n",
    "    for script in tqdm(df[column_name], desc=\"Extracting Tokens\"):\n",
    "        token_lists.append(extract_token(str(script)))\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(token_lists, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    print(f\"‚úÖ ƒê√£ hu·∫•n luy·ªán xong Token2Vec v·ªõi {len(model.wv.index_to_key)} tokens.\")\n",
    "\n",
    "    # Save model\n",
    "    model.save(model_path)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u model t·∫°i {model_path}\")\n",
    "\n",
    "    # Embed t·ª´ng d√≤ng n·∫øu thi·∫øu Token2Vec\n",
    "    for idx in tqdm(range(len(df)), desc=\"Embedding missing Token2Vec\"):\n",
    "        if pd.notna(df.at[idx, 'Token2Vec']):\n",
    "            # N·∫øu record ƒë√£ c√≥ vector th√¨ skip\n",
    "            continue\n",
    "\n",
    "        tokens = token_lists[idx]\n",
    "        if tokens:\n",
    "            vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "            avg_vector = np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "        else:\n",
    "            avg_vector = np.zeros(vector_size)\n",
    "\n",
    "        vec_str = ' '.join(map(str, avg_vector))\n",
    "        df.at[idx, 'Token2Vec'] = vec_str\n",
    "\n",
    "    # Ghi ƒë√® l·∫°i CSV\n",
    "    df.to_csv(input_csv, index=False)\n",
    "    print(f\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t v√† l∆∞u l·∫°i {input_csv} v·ªõi Token2Vec cho c√°c record c√≤n thi·∫øu.\")\n",
    "\n",
    "# ===== Main processing =====\n",
    "\n",
    "input_csv = './dataset/data_labeled_extended.csv'\n",
    "\n",
    "generate_token2vec(input_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66dc19",
   "metadata": {},
   "source": [
    "## ast2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be43761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from anytree import Node\n",
    "import logging\n",
    "\n",
    "# C√†i ƒë·∫∑t logging ƒë·ªÉ theo d√µi c√°c th√¥ng b√°o c·∫£nh b√°o\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== Helper Classes and Functions =====\n",
    "\n",
    "class EnhancedPowerShellASTParser:\n",
    "    def __init__(self, code):\n",
    "        self.code = code\n",
    "        self.root = Node(\"Root\")\n",
    "\n",
    "    def parse_code(self):\n",
    "        if not isinstance(self.code, str):  # Ki·ªÉm tra xem code c√≥ ph·∫£i l√† chu·ªói kh√¥ng\n",
    "            logger.warning(f\"Invalid input for AST parsing: {self.code}\")\n",
    "            return self.root  # Ho·∫∑c tr·∫£ v·ªÅ c√¢y r·ªóng n·∫øu c·∫ßn\n",
    "\n",
    "        tokens = re.findall(r'\\b\\w+\\b', self.code)\n",
    "        current_node = self.root\n",
    "        for token in tokens:\n",
    "            if token.lower() in [\"function\", \"if\", \"foreach\", \"while\", \"try\", \"catch\"]:\n",
    "                Node(f\"{token.capitalize()}Statement\", parent=current_node)\n",
    "            elif token.lower() in [\"param\", \"return\"]:\n",
    "                Node(\"ParameterStatement\", parent=current_node)\n",
    "            elif token.lower() == \"pipeline\":\n",
    "                Node(\"Pipeline\", parent=current_node)\n",
    "            elif token.lower() in [\"cmdlet\", \"assignment\"]:\n",
    "                Node(f\"{token.capitalize()}Node\", parent=current_node)\n",
    "            else:\n",
    "                Node(token, parent=current_node)\n",
    "        return self.root\n",
    "\n",
    "    def postorder_traversal(self, node):\n",
    "        sequence = []\n",
    "        for child in node.children:\n",
    "            sequence.extend(self.postorder_traversal(child))\n",
    "        sequence.append(node.name)\n",
    "        return sequence\n",
    "\n",
    "def calculate_word_vectors(features, vector_size=128, window=5, min_count=1):\n",
    "    model = Word2Vec(sentences=features, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    return model\n",
    "\n",
    "def generate_ast_embedding(ast_sequence, model, vector_size=128):\n",
    "    vectors = [model.wv[token] for token in ast_sequence if token in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# ===== Main Processing =====\n",
    "\n",
    "def generate_ast2vec(input_csv, column_name='Full Payload', label_column='Label', vector_size=128):\n",
    "    model_dir = './vectorization/ast2vec'\n",
    "    model_path = os.path.join(model_dir, 'ast2vec_extended.model')\n",
    "\n",
    "    # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {input_csv}: {e}\")\n",
    "        return\n",
    "\n",
    "    if column_name not in df.columns or label_column not in df.columns:\n",
    "        raise ValueError(f\"'{column_name}' ho·∫∑c '{label_column}' kh√¥ng t·ªìn t·∫°i trong file CSV.\")\n",
    "\n",
    "    # L·ªçc b·ªè c√°c d√≤ng c√≥ gi√° tr·ªã kh√¥ng ph·∫£i chu·ªói trong c·ªôt 'Full Payload'\n",
    "    df = df[df[column_name].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
    "\n",
    "    # N·∫øu ch∆∞a c√≥ c·ªôt Ast2Vec ‚ûî t·∫°o c·ªôt tr·ªëng\n",
    "    if 'Ast2Vec' not in df.columns:\n",
    "        df['Ast2Vec'] = np.nan\n",
    "\n",
    "    print(\"‚è≥ ƒêang ti·∫øn h√†nh build ho·∫∑c update Ast2Vec...\")\n",
    "\n",
    "    # Parse AST cho to√†n b·ªô payloads\n",
    "    ast_sequences = []\n",
    "    for script in tqdm(df[column_name], desc=\"Parsing AST\"):\n",
    "        parser = EnhancedPowerShellASTParser(script)\n",
    "        ast_tree = parser.parse_code()\n",
    "        sequence = parser.postorder_traversal(ast_tree)\n",
    "        ast_sequences.append(sequence)\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    model = calculate_word_vectors(ast_sequences, vector_size=vector_size)\n",
    "    print(f\"‚úÖ ƒê√£ hu·∫•n luy·ªán xong AST2Vec v·ªõi {len(model.wv.index_to_key)} tokens.\")\n",
    "\n",
    "    # Save model\n",
    "    model.save(model_path)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u model t·∫°i {model_path}\")\n",
    "\n",
    "    # Embed t·ª´ng d√≤ng n·∫øu thi·∫øu Ast2Vec\n",
    "    for idx in tqdm(range(len(df)), desc=\"Embedding missing Ast2Vec\"):\n",
    "        if pd.notna(df.at[idx, 'Ast2Vec']):\n",
    "            continue\n",
    "\n",
    "        sequence = ast_sequences[idx]\n",
    "        vec = generate_ast_embedding(sequence, model, vector_size=vector_size)\n",
    "        vec_str = ' '.join(map(str, vec))\n",
    "        df.at[idx, 'Ast2Vec'] = vec_str\n",
    "\n",
    "    # Ghi ƒë√® l·∫°i CSV\n",
    "    df.to_csv(input_csv, index=False)\n",
    "    print(f\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t v√† l∆∞u l·∫°i {input_csv} v·ªõi Ast2Vec cho c√°c record c√≤n thi·∫øu.\")\n",
    "\n",
    "# ===== Example usage =====\n",
    "input_csv = './dataset/data_labeled_extended.csv'\n",
    "generate_ast2vec(input_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85263ca",
   "metadata": {},
   "source": [
    "# processing and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61193cc6",
   "metadata": {},
   "source": [
    "## fussion to 1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7666900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== G·ªôp c√°c vector t·ª´ data_labeled.csv th√†nh FusionVector 512 chi·ªÅu ======\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== Config paths =====\n",
    "input_csv = './dataset/data_labeled_extended.csv'\n",
    "output_csv = './dataset/fusion_with_vector_extended.csv'\n",
    "\n",
    "# ===== Load dataset =====\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_csv)\n",
    "except Exception as e:\n",
    "    print(f\"L·ªói khi load {input_csv}: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ load {len(df)} d√≤ng t·ª´ {input_csv}\")\n",
    "\n",
    "# ===== Ki·ªÉm tra b·∫Øt bu·ªôc =====\n",
    "\n",
    "required_columns = ['Char2Vec', 'Token2Vec', 'Rela2Vec', 'Ast2Vec']\n",
    "\n",
    "missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è Thi·∫øu c√°c c·ªôt vector sau: {missing_cols}. D·ª´ng ti·∫øn tr√¨nh.\")\n",
    "    exit()\n",
    "\n",
    "print(\"‚úÖ ƒê√£ x√°c nh·∫≠n ƒë·ªß 4 c·ªôt vector c·∫ßn thi·∫øt.\")\n",
    "\n",
    "# ===== Parse c√°c vector t·ª´ chu·ªói sang numpy array =====\n",
    "\n",
    "def parse_vector_string(vec_string):\n",
    "    try:\n",
    "        return np.fromstring(vec_string, sep=' ')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing vector: {e}\")\n",
    "        return np.zeros(128)  # fallback an to√†n\n",
    "\n",
    "for col in required_columns:\n",
    "    array_col = col + '_array'\n",
    "    df[array_col] = df[col].apply(parse_vector_string)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ parse 4 vector th√†nh np.array.\")\n",
    "\n",
    "# ===== G·ªôp c√°c vector th√†nh FusionVector 512 chi·ªÅu =====\n",
    "\n",
    "def fuse_vectors(row):\n",
    "    try:\n",
    "        return np.concatenate([\n",
    "            row['Char2Vec_array'],\n",
    "            row['Token2Vec_array'],\n",
    "            row['Rela2Vec_array'],\n",
    "            row['Ast2Vec_array']\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fusing vectors at row: {e}\")\n",
    "        return np.zeros(512)\n",
    "\n",
    "df['FusionVector'] = tqdm(df.apply(fuse_vectors, axis=1), total=len(df), desc=\"Fusion vectors\")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ g·ªôp th√†nh FusionVector (512 chi·ªÅu).\")\n",
    "\n",
    "# ===== L∆∞u l·∫°i file m·ªõi =====\n",
    "\n",
    "# Convert FusionVector th√†nh chu·ªói ƒë·ªÉ l∆∞u CSV\n",
    "df['FusionVector'] = df['FusionVector'].apply(lambda vec: ' '.join(map(str, vec)))\n",
    "\n",
    "# Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c·∫ßn thi·∫øt\n",
    "columns_to_keep = ['Full Payload', 'Label', 'FusionVector']\n",
    "fusion_df = df[columns_to_keep]\n",
    "\n",
    "os.makedirs('./dataset', exist_ok=True)\n",
    "fusion_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u file {output_csv} th√†nh c√¥ng.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405f11a",
   "metadata": {},
   "source": [
    "## load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_for_training.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== Config paths =====\n",
    "\n",
    "fusion_with_vector_csv = './dataset/fusion_with_vector_extended.csv'\n",
    "\n",
    "# ===== Load dataset =====\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(fusion_with_vector_csv)\n",
    "except Exception as e:\n",
    "    print(f\"L·ªói khi load fusion_with_vector_extended.csv: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ load {len(df)} d√≤ng t·ª´ {fusion_with_vector_csv}\")\n",
    "\n",
    "# ===== Parse FusionVector t·ª´ chu·ªói sang np.array =====\n",
    "\n",
    "def parse_fusion_vector(vec_string):\n",
    "    try:\n",
    "        return np.fromstring(vec_string, sep=' ')\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing fusion vector: {e}\")\n",
    "        return np.zeros(512)  # fallback n·∫øu l·ªói\n",
    "\n",
    "df['FusionVector_array'] = df['FusionVector'].apply(parse_fusion_vector)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ parse FusionVector th√†nh np.array.\")\n",
    "\n",
    "# ===== Chu·∫©n b·ªã X, y =====\n",
    "\n",
    "# X l√† FusionVector, y l√† Label\n",
    "X = np.stack(df['FusionVector_array'].values)\n",
    "y_labels = df['Label'].values\n",
    "\n",
    "print(f\"‚úÖ X c√≥ shape {X.shape}\")\n",
    "print(f\"‚úÖ y c√≥ shape {y_labels.shape}\")\n",
    "\n",
    "# ===== One-hot encode Label =====\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y_labels)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ one-hot encode Label th√†nh shape {y.shape}\")\n",
    "\n",
    "# ===== Reshape X cho CNN/BiLSTM =====\n",
    "\n",
    "# Reshape (batch_size, 512, 1) cho 1D CNN ho·∫∑c BiLSTM input\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(f\"‚úÖ X sau reshape c√≥ shape {X.shape}\")\n",
    "\n",
    "# ===== Split train/test =====\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ chia d·ªØ li·ªáu: Train {X_train.shape[0]} samples, Test {X_test.shape[0]} samples.\")\n",
    "\n",
    "# ===== Visualization: S·ªë l∆∞·ª£ng m·∫´u train/test =====\n",
    "\n",
    "# Data for bar chart\n",
    "data = {\n",
    "    'Dataset': ['X_train', 'y_train', 'X_test', 'y_test'],\n",
    "    'Number of Samples': [X_train.shape[0], y_train.shape[0], X_test.shape[0], y_test.shape[0]]\n",
    "}\n",
    "\n",
    "df_vis = pd.DataFrame(data)\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(df_vis['Dataset'], df_vis['Number of Samples'], color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Number of Samples in Train and Test Sets')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# ===== In l·∫°i ki·ªÉm tra cu·ªëi =====\n",
    "\n",
    "print(\"====== FINAL CHECK ======\")\n",
    "print(f\"Type X_train: {type(X_train)}, shape: {X_train.shape}\")\n",
    "print(f\"Type y_train: {type(y_train)}, shape: {y_train.shape}\")\n",
    "print(f\"Type X_test: {type(X_test)}, shape: {X_test.shape}\")\n",
    "print(f\"Type y_test: {type(y_test)}, shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872aea5c",
   "metadata": {},
   "source": [
    "## train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== train_cnn_bilstm_attention.py ======\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# ===== Config =====\n",
    "INPUT_SHAPE = (512, 1)   # V√¨ fusion vector c√≥ 512 chi·ªÅu\n",
    "NUM_CLASSES = 5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# ===== Build Model =====\n",
    "def build_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(input_layer)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(0.001)))(x)\n",
    "\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===== Train Model =====\n",
    "def train_model(model, X_train, y_train):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split=0.2,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=1)\n",
    "    return history\n",
    "\n",
    "# ===== Evaluate Model =====\n",
    "def evaluate_model(model, X_test, y_test, target_names):\n",
    "    print(\"\\n=== Evaluating Model ===\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    if y_test.shape[1] > 1:\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_test_labels = y_test\n",
    "\n",
    "    precision = precision_score(y_test_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(y_test_labels, y_pred_labels)\n",
    "\n",
    "    print(f\"‚úÖ Accuracy: {acc:.4f}\")\n",
    "    print(f\"‚úÖ Precision: {precision:.4f}\")\n",
    "    print(f\"‚úÖ Recall: {recall:.4f}\")\n",
    "    print(f\"‚úÖ F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    labels_in_test = sorted(list(unique_labels(y_test_labels, y_pred_labels)))\n",
    "    target_names_subset = [target_names[i] for i in labels_in_test]\n",
    "    print(classification_report(y_test_labels, y_pred_labels, target_names=target_names_subset, digits=4, zero_division=0))\n",
    "\n",
    "# ===== Plot Training Curves =====\n",
    "def plot_training_curves(history):\n",
    "    print(\"\\n=== Plotting Training Curves ===\")\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===== Main Process =====\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_csv('./dataset/fusion_with_vector_extended.csv')\n",
    "\n",
    "    def parse_fusion_vector(vec_string):\n",
    "        return np.fromstring(vec_string, sep=' ')\n",
    "\n",
    "    df['FusionVector_array'] = df['FusionVector'].apply(parse_fusion_vector)\n",
    "\n",
    "    X = np.stack(df['FusionVector_array'].values)\n",
    "    y_labels = df['Label'].values\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    y = lb.fit_transform(y_labels)\n",
    "\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "    # --- Chia train/test c√≥ stratify ƒë·ªÉ kh√¥ng m·∫•t l·ªõp nh·ªè ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y)\n",
    "\n",
    "    target_names = lb.classes_.tolist()\n",
    "\n",
    "    # Build, Train, Evaluate\n",
    "    model = build_model()\n",
    "    history = train_model(model, X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test, target_names)\n",
    "    plot_training_curves(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
